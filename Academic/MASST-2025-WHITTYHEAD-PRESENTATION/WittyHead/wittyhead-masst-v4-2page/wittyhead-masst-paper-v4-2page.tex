\documentclass[conference]{IEEEtran}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{textcomp}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}

\title{WittyHead: A Multi-Modal Architecture for Empathetic Human-Agent Collaboration Through Emotional Expressivity}

\author{\IEEEauthorblockN{Yuri A. Tijerino}
\IEEEauthorblockA{\textit{Kwansei Gakuin University} \\
\textit{Intelligent Blockchain+ Innovation Research Center}\\
Sanda, Hyogo, Japan \\
ontologist@kwansei.ac.jp}
}

\maketitle

\begin{abstract}
Multi-agent systems supporting vulnerable populations require authentic emotional expression beyond conversational capability. This paper presents WittyHead, an anthropomorphic empathetic agent with coordinated facial expressions, gaze, gestures, and voice. WittyHead addresses three MASST priorities: (1) context-aware behavioral guard rails through ontology-driven emotion validation, (2) mutual observability through explainable multi-modal reasoning, and (3) design-time risk mitigation through therapeutic alliance research integration. Based on evidence that empathetic responses require compassionate concern rather than emotional mirroring \cite{gilbert2019compassion}, WittyHead implements asymmetric response mappings coordinating FACS-validated facial expressions, therapeutic eye contact, and prosodic-aligned gestures for privacy-preserving community support through Digital MOAI networks.
\end{abstract}

\begin{IEEEkeywords}
multi-agent safety, empathetic agents, emotional expressivity, therapeutic alliance, behavioral guard rails
\end{IEEEkeywords}

\section{Introduction}

Multi-agent systems (MAS) serving vulnerable populations face critical safety requirements: preventing inappropriate emotional responses, maintaining transparent reasoning, and mitigating design-time risks \cite{bradshaw2024masst}. Current empathetic agents exhibit a fundamental flaw: simple emotional mirroring that can exacerbate user distress \cite{gilbert2019compassion}. Therapeutic alliance research demonstrates that empathetic responses to distress require compassionate concern, not mirrored negative emotions---smiling at someone experiencing distress is perceived as ``invalidating and aversive'' \cite{gilbert2019compassion}.

WittyHead is an anthropomorphic multi-modal empathetic agent implementing evidence-based emotional expressivity through coordinated facial expressions (FACS), gaze (therapeutic alliance), gestures (prosodic-aligned), and voice modulation. The system serves as the empathetic interface for Digital MOAI \cite{buettner2008blue}, AI-enhanced mutual aid networks for vulnerable populations including foster care youth, elderly, and individuals experiencing mental health challenges.

\section{MASST-Aligned Architecture}

\subsection{Context-Aware Behavioral Guard Rails}

WittyHead implements three layers of behavioral safety:

\textbf{Empathetic Response Mapping:} Asymmetric emotion mappings prevent invalidating mirroring. Table \ref{tab:empathy-map} shows evidence-based mappings where user distress (sad, angry, fear) triggers compassionate concern, not mirrored distress. This implements Gilbert et al.'s finding that compassionate expressions achieve significantly higher empathy ratings than mirrored negative emotions \cite{gilbert2019compassion}.

\begin{table}[h]
\centering
\caption{Empathetic Response Mapping}
\begin{tabular}{|l|l|l|}
\hline
\textbf{User Emotion} & \textbf{Avatar Response} & \textbf{Safety Rationale} \\
\hline
Sad & Compassionate Concern & Prevent mirroring \\
Angry & Calm Concern & De-escalation \\
Fear & Reassuring & Protective stability \\
Happy & Happy & Reinforce positive \\
\hline
\end{tabular}
\label{tab:empathy-map}
\end{table}

\textbf{Ontology-Driven Validation:} Medical ontologies block inappropriate responses (e.g., blocking celebratory expressions for serious diagnoses). Domain-specific ontologies provide contextual appropriateness checks before expression generation.

\textbf{Accessibility Guard Rails:} WCAG 2.1 AAA compliance \cite{w3c2018wcag} ensures expressions accommodate visual, auditory, motor, and cognitive disabilities.

\subsection{Mutual Observability}

WittyHead provides explainable multi-modal reasoning enabling human oversight:

\textbf{Transparency:} System logs specify: ``User emotion: Sad (0.8 intensity) → Avatar response: Compassionate Concern with 75\% eye contact (therapeutic alliance), palm-up gestures (openness), FACS AU4+AU6+AU12 (concern+warmth).''

\textbf{Multi-Modal Coordination:} Six coordinated services ensure synchronized expressivity: (1) Emotion Detection (audio prosody, facial analysis), (2) Empathetic Response Orchestrator (asymmetric mappings), (3) Facial Expression Manager (FACS-validated ARKit blendshapes), (4) Gaze Manager (therapeutic eye contact \cite{dowell2013therapist}), (5) Gesticulation Manager (prosodic-aligned gestures \cite{loehr2012temporal}), (6) Voice Modulation (prosody control). Central orchestrator ensures 60 FPS synchronization (16.7ms precision).

\textbf{Reasoning Traces:} Decisions traceable to evidence: compassionate concern for sadness cites Gilbert et al. \cite{gilbert2019compassion}, therapeutic eye contact cites therapeutic alliance research \cite{dowell2013therapist}, palm-up gestures cite social signaling research \cite{pease2006definitive}.

\subsection{Design-Time Risk Mitigation}

WittyHead integrates therapeutic alliance research preventing empathy failures before deployment:

\textbf{FACS-Validated Expressions:} Facial Action Coding System \cite{ekman1978facial} provides scientific basis for expressions. Compassionate concern implements AU4 (brow lowerer) + AU6 (cheek raiser) + gentle AU12 (lip corner puller), validated by Gilbert et al. \cite{gilbert2019compassion} as conveying understanding without sharing distress.

\textbf{Therapeutic Eye Contact:} Research demonstrates that high eye contact enhances therapeutic alliance and empathy \cite{dowell2013therapist}. Approach-avoidance theory \cite{adams2005effects} informs gaze direction for different emotional contexts.

\textbf{Prosodic Gesture Alignment:} Gestures synchronize with prosodic peaks \cite{loehr2012temporal}, not keywords. Palm orientation signals trust (palm-up for empathy) vs. dominance (palm-down avoided) \cite{pease2006definitive}.

\section{Digital MOAI Integration}

WittyHead provides the empathetic interface for Digital MOAI, AI-enhanced Okinawan mutual aid networks (模合, moai) serving vulnerable populations. Traditional MOAI (5-person lifelong support collectives) demonstrate effectiveness through Okinawa's world-highest centenarian concentration \cite{buettner2008blue}.

\textbf{Privacy-Preserving:} Local-first emotion processing on AIngle DLT platform (EU H2020 FASTER project \cite{faster2019h2020}), no centralized data collection. 0.16ms latency supports real-time expressivity.

\textbf{User-Controlled:} Three automation levels (propose, notify, act independently) provide explicit consent for AI emotional responses. Group-aware context enables culturally-appropriate expressions for MOAI communities.

\textbf{Safety-Critical Application:} Foster care youth, elderly experiencing social isolation, mental health support between therapy sessions. AI applications for vulnerable populations in healthcare contexts have been explored \cite{kotoku2021ai}.

\section{Implementation}

Core innovation is non-mirroring empathetic response architecture implementing therapeutic research findings:

\textbf{Compassionate Concern Expression:} ARKit blendshapes implement validated FACS pattern. browDownLeft/Right: 0.4 (AU4 concern), browInnerUp: 0.3 (attention), cheekSquint: 0.3 (AU6 warmth), mouthSmile: 0.2 (gentle AU12), relaxed lower face (not tense). This pattern conveys understanding without sharing user's distress \cite{gilbert2019compassion}.

\textbf{Multi-Modal Coordination:} (1) Facial-Gaze: Direct gaze enhances approach emotions, averted enhances avoidance \cite{adams2005effects}. (2) Gesture-Prosody: Beat gestures on pitch peaks \cite{loehr2012temporal}. (3) Voice-Facial: Vocal pitch correlates with facial intensity. (4) Gaze-Turn-Taking: Eye contact marks turn-yielding \cite{vertegaal2001eye}.

\section{Discussion}

\subsection{MASST Initiative Contributions}

WittyHead demonstrates three safety mechanisms for empathetic MAS:

\textbf{Behavioral Guard Rails:} Asymmetric emotion mappings prevent invalidating mirroring documented to harm vulnerable users \cite{gilbert2019compassion}. Ontology-driven validation blocks contextually inappropriate responses. Accessibility guard rails ensure inclusive emotional support.

\textbf{Mutual Observability:} Explainable multi-modal reasoning enables human oversight. Decisions traceable to therapeutic alliance research \cite{dowell2013therapist,gilbert2019compassion}. Transparent logs facilitate accountability.

\textbf{Design-Time Mitigation:} Integration of validated therapeutic alliance research \cite{gilbert2019compassion,dowell2013therapist}, FACS \cite{ekman1978facial}, gesture-prosody coordination \cite{loehr2012temporal} prevents empathy failures before deployment. Evidence-based design reduces uncanny valley risks.

\subsection{Implications for Empathetic MAS}

Beyond emotional mirroring paradigm: Empathy requires different strategies for positive vs. negative emotions. Mirrored distress undermines trust with vulnerable populations \cite{gilbert2019compassion}. WittyHead demonstrates multi-modal coordination can implement therapeutic alliance principles for safe, effective empathetic agents.

Digital Therapeutic Alliance (DTA) extension: While DTA research focuses on text-based chatbots \cite{beatty2022wysa,dalfonso2020digital}, WittyHead extends to multi-modal embodied agents. Coordinated nonverbal cues (facial, gaze, gesture) grounded in face-to-face therapeutic research enable anthropomorphic empathetic expressivity beyond conversational agents.

\section{Conclusion}

WittyHead demonstrates that safe empathetic multi-agent systems require: (1) behavioral guard rails preventing invalidating emotional mirroring, (2) explainable multi-modal reasoning enabling human oversight, and (3) design-time risk mitigation through therapeutic alliance research integration. Evidence that empathy requires compassionate concern rather than mirroring \cite{gilbert2019compassion} informs asymmetric response mappings coordinating FACS-validated expressions, therapeutic eye contact, and prosodic gestures. Integration with Digital MOAI demonstrates privacy-preserving community support for vulnerable populations. Future work will validate through human subjects research (JSPS KAKENHI Grant JP23K01882) employing DTA measurement instruments \cite{beatty2022wysa} to assess anthropomorphic avatar alliance beyond text-based agents.

\section*{Acknowledgments}
Supported by JSPS KAKENHI Grant JP23K01882 (PI: K. Kotoku). AIngle DLT by EU H2020 FASTER (Grant 833507). Thanks to K. Kotoku (Fukuoka University) and Y. Miyazaki (Kwansei Gakuin University).

\bibliographystyle{IEEEtran}
\bibliography{references-v4}

\end{document}
